{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "import sys\n",
    "from torch import nn\n",
    "import sklearn\n",
    "import pathlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.special import rel_entr\n",
    "from textblob import TextBlob    \n",
    "import cv2\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "        \n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../utils\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../llava\")))\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, process_images, KeywordsStoppingCriteria\n",
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from transformers import set_seed\n",
    "from notebooks.utils.train_utils import get_answer_with_probability, calculate_distance\n",
    "from notebooks.utils.diffusion_noise import add_diffusion_noise\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.37s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "conv_mode='llava_v1'\n",
    "cuda_device='3'\n",
    "cd_beta = 0.1\n",
    "cd_alpha = 1\n",
    "temperature = 1.0\n",
    "top_p = 1\n",
    "top_k = None\n",
    "device = torch.device(\"cuda:\" + str(cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = os.path.expanduser(model_name)\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, None, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1075it [52:12,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from collections import defaultdict, Counter\n",
    "from evaluate import load\n",
    "import ast\n",
    "import re\n",
    "from notebooks.utils.train_utils import get_answer_with_probability, calculate_distance, get_answer_with_probability_beams\n",
    "from notebooks.utils.LAVE.eval_utils import lave_scorer\n",
    "N = 0\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "masking_type = 'Diffusion'\n",
    "noise_step = 900\n",
    "\n",
    "# Save the generation if post processing needed\n",
    "pathlib.Path().mkdir(parents=True, exist_ok=True)\n",
    "answers_file = '/vault/erum/generations/llava7b_aokvqa_generation.json'\n",
    "os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n",
    "# ans_file = open(answers_file, \"w\")\n",
    "sequences = []\n",
    "vcd_confidence_scores = []\n",
    "noise_confidence_scores =  []\n",
    "accQA = []\n",
    "acc = 0.0\n",
    "\n",
    "#AOKVQA\n",
    "data_dir = '/vault/erum/AOKVQA'\n",
    "attn_dir =  '/vault/erum/AOKVQA/attn_val2017/'\n",
    "images_dir = '/vault/erum/AOKVQA/val2017/'\n",
    "split = 'val'\n",
    "data_file = os.path.join(data_dir, f\"aokvqa_v1p0_{split}.json\")\n",
    "data = json.load(open(data_file))\n",
    "data = [d for d in data if d['difficult_direct_answer'] is False]\n",
    "image_filenames = os.listdir(images_dir)\n",
    "imageid2filename = {}\n",
    "for fn in image_filenames:\n",
    "    image_id = int(fn.split('_')[-1].strip('.jpg'))\n",
    "    imageid2filename[image_id] = os.path.join(images_dir, fn)\n",
    "imageids = list(set(list(imageid2filename.keys())))\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "directvqa_rollouts = defaultdict(dict)\n",
    "\n",
    "N = 0\n",
    "for i, datum in tqdm(enumerate(data)):   \n",
    "    save_regdistributiontop5 = []\n",
    "    save_augdistributiontop5 = []\n",
    "    save_regdistributiontop2 = []\n",
    "    save_augdistributiontop2 = []\n",
    "    N += 1\n",
    "    topk_distance = []\n",
    "\n",
    "    image_filename = imageid2filename[datum['image_id']]\n",
    "    qs = datum['question']\n",
    "    answers_dict = datum['direct_answers']\n",
    "    image_file = Image.open(image_filename).convert('RGB')\n",
    "\n",
    "    answer_counter = Counter(answers_dict)\n",
    "\n",
    "    score_dict = defaultdict(float)\n",
    "    for a, c in answer_counter.items():\n",
    "        score_dict[a] = min(1, c/3.0)\n",
    "\n",
    "    augmented_rephrase_list = []\n",
    "    reg_rephrase_list = []\n",
    "    logits_aug2 = []\n",
    "    \n",
    "    cur_prompt = datum['question']\n",
    "    prompt = f\"USER: <image>\\nAnswer the question using a single word or phrase: {cur_prompt} ASSISTANT:\"\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device=device, non_blocking=True)\n",
    "    image_tensor = process_images([image_file], image_processor, model.config)[0]\n",
    "        \n",
    "    \n",
    "    if masking_type == 'Diffusion': \n",
    "        image_tensor_cd = add_diffusion_noise(image_tensor, noise_step)\n",
    "    elif masking_type == 'attention':\n",
    "        attn_filename = os.path.join(attn_dir, str(datum['image_id'])+'.jpg')\n",
    "        image_file_attn = Image.open(attn_filename).convert('RGB')\n",
    "        image_tensor_cd = process_images([image_file_attn], image_processor, model.config)[0]\n",
    "    elif masking_type == 'black':\n",
    "        image_tensor_cd = Image.open('black.png').convert('RGB')\n",
    "        image_tensor_cd = image_tensor_cd.convert('RGB')\n",
    "        image_tensor_cd = process_images([image_tensor_cd], image_processor, model.config)[0]\n",
    "    else:\n",
    "        image_tensor_cd = None\n",
    "\n",
    "    image_tensor_cd2 = Image.open('black.png').convert('RGB')\n",
    "    image_tensor_cd2 = image_tensor_cd2.convert('RGB')\n",
    "    image_tensor_cd2 = process_images([image_tensor_cd2], image_processor, model.config)[0]\n",
    "\n",
    "    attn_filename = os.path.join(attn_dir, str(datum['image_id'])+'.jpg')\n",
    "    image_file_attn = Image.open(attn_filename).convert('RGB')\n",
    "    image_tensor_cd3 = process_images([image_file_attn], image_processor, model.config)[0]\n",
    "\n",
    "    # Black Image forward Pass\n",
    "    with torch.inference_mode():\n",
    "        outputs_aug2 = model.generate(input_ids, \n",
    "        images=image_tensor_cd2.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1024, return_dict_in_generate=True, output_scores=True, output_hidden_states=True)\n",
    "    blackimage_logits = torch.cat(outputs_aug2.scores)  #logits\n",
    "    blackimage_probs = nn.Softmax(dim=-1)(blackimage_logits)\n",
    "    blackimage_generated_ids = outputs_aug2.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "    # attention based masking forward pass\n",
    "    with torch.inference_mode():\n",
    "        outputs_aug3 = model.generate(input_ids, \n",
    "        images=image_tensor_cd3.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1024, return_dict_in_generate=True, output_scores=True)\n",
    "    attn_logits = torch.cat(outputs_aug3.scores)  #logits\n",
    "    attn_probs = nn.Softmax(dim=-1)(attn_logits)\n",
    "    attn_generated_ids = outputs_aug3.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "    # Diffusion Noise\n",
    "    with torch.inference_mode():\n",
    "        outputs_aug = model.generate(input_ids, \n",
    "        images=image_tensor_cd.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1024, return_dict_in_generate=True, output_scores=True)\n",
    "    diff_logits = torch.cat(outputs_aug.scores)  #logits\n",
    "    diff_probs = nn.Softmax(dim=-1)(diff_logits)\n",
    "    diff_generated_ids = outputs_aug.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "    # Greedy search Original image\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(input_ids, \n",
    "        images=image_tensor.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1024,  return_dict_in_generate=True, output_scores=True)\n",
    "    logits = torch.cat(outputs.scores)\n",
    "    token_probs = nn.Softmax(dim=-1)(logits)\n",
    "    generated_ids = outputs.sequences[:, input_ids.shape[-1]:]\n",
    "\n",
    "    flattened_values = []\n",
    "    for ii in range(token_probs.shape[0]):\n",
    "        flattened_values.append(float(token_probs[ii][generated_ids[0][ii]].cpu()))\n",
    "\n",
    "    blackimage_flattened_values = []\n",
    "    for ii in range(blackimage_probs.shape[0]):\n",
    "        if generated_ids[0].shape[0] > ii:\n",
    "            blackimage_flattened_values.append(float(blackimage_probs[ii][generated_ids[0][ii]].cpu()))\n",
    "\n",
    "    attn_flattened_values = []\n",
    "    for ii in range(attn_probs.shape[0]):\n",
    "        if generated_ids[0].shape[0] > ii:\n",
    "            attn_flattened_values.append(float(attn_probs[ii][generated_ids[0][ii]].cpu()))           \n",
    "\n",
    "    diff_flattened_values = []\n",
    "    for ii in range(diff_probs.shape[0]):\n",
    "        if generated_ids[0].shape[0] > ii:\n",
    "            diff_flattened_values.append(float(diff_probs[ii][generated_ids[0][ii]].cpu()))           \n",
    "\n",
    "    # Log prob\n",
    "    answer, decoded_token, logprobs_dict = get_answer_with_probability( \n",
    "            flattened_values, \n",
    "            generated_ids[0].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "    # Greedy search\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(input_ids, \n",
    "        images=image_tensor.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        num_beams=5, max_new_tokens=1024, num_return_sequences=5, return_dict_in_generate=True, output_scores=True)\n",
    "    generated_ids_ = outputs.sequences[:, input_ids.shape[-1]:]\n",
    "    transition_scores_ = model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=True\n",
    "        )\n",
    "    beam_answers = []\n",
    "    beam_logprobs = []\n",
    "    beam_answers.append(answer)\n",
    "    beam_logprobs.append(logprobs_dict)\n",
    "    for beam in range(5):\n",
    "        beam_answer, decoded_tokens, beam_logprobs_dict = get_answer_with_probability_beams(\n",
    "            transition_scores_[beam].tolist(), \n",
    "            generated_ids_[beam].tolist(), \n",
    "            tokenizer)\n",
    "        beam_answers.append(beam_answer)\n",
    "        beam_logprobs.append(beam_logprobs_dict)\n",
    "\n",
    "    _ , _ , blackimage_logprobs_dict = get_answer_with_probability( \n",
    "            blackimage_flattened_values, \n",
    "            generated_ids[0][:blackimage_probs.shape[0]].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "    blackimage_answer , _ , _ = get_answer_with_probability( \n",
    "            blackimage_flattened_values, \n",
    "            blackimage_generated_ids[0][:blackimage_probs.shape[0]].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "    _ , _, attn_logprobs_dict = get_answer_with_probability( \n",
    "            attn_flattened_values, \n",
    "            generated_ids[0][:attn_probs.shape[0]].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "    _ , _, diff_logprobs_dict = get_answer_with_probability( \n",
    "            diff_flattened_values, \n",
    "            generated_ids[0][:diff_probs.shape[0]].tolist(),\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "    k = 5\n",
    "    j = 0\n",
    "    values, indices5 = torch.topk(token_probs, 5)\n",
    "    values, indices2 = torch.topk(token_probs, 2)  \n",
    "\n",
    "    distance_reg_black = calculate_distance(np.array(token_probs[j][indices5[j]].cpu().numpy()), np.array(blackimage_probs[j][indices5[j]].cpu().numpy()), np.array(token_probs[j][indices2[j]].cpu().numpy()), np.array(blackimage_probs[j][indices2[j]].cpu().numpy()), [])\n",
    "    distance_reg_attn = calculate_distance(np.array(token_probs[j][indices5[j]].cpu().numpy()), np.array(attn_probs[j][indices5[j]].cpu().numpy()), np.array(token_probs[j][indices2[j]].cpu().numpy()), np.array(attn_probs[j][indices2[j]].cpu().numpy()), [])\n",
    "    distance_reg_diff = calculate_distance(np.array(token_probs[j][indices5[j]].cpu().numpy()), np.array(diff_probs[j][indices5[j]].cpu().numpy()), np.array(token_probs[j][indices2[j]].cpu().numpy()), np.array(diff_probs[j][indices2[j]].cpu().numpy()), [])\n",
    "\n",
    "\n",
    "    lave_reasoning, lave_score = lave_scorer.compute(\n",
    "    prediction=answer,\n",
    "    references=answers_dict,\n",
    "    question=qs,\n",
    "    )\n",
    "\n",
    "    #self eval\n",
    "    yn_tokens = tokenizer.tokenize(\"yesYes Yesyesno noNo No\")\n",
    "    yn_token_ids = tokenizer.convert_tokens_to_ids(yn_tokens)\n",
    "    yes_token_ids = yn_token_ids[:4]\n",
    "    no_token_ids = yn_token_ids[4:]\n",
    "\n",
    "    self_prompting_confidence_reg = []\n",
    "    self_prompting_confidence_aug = []\n",
    "\n",
    "\n",
    "    cur_prompt = datum['question']\n",
    "    eval_prompt = \"USER: <image> Question: In this image, \"+ cur_prompt+ \"\\nAnswer: \" + answer + \"\\nIs the given answer correct for the question based on the given image? Options: yes, no. ASSISTANT:\"\n",
    "    inputs = tokenizer_image_token(eval_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device=device, non_blocking=True)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs_ = model.generate(inputs, \n",
    "        images=image_tensor.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    logits = outputs_.scores[0]\n",
    "\n",
    "    token_probs = nn.Softmax(dim=-1)(logits)\n",
    "    values, indices = torch.topk(token_probs, 15)\n",
    "    ids = [x for x in indices]\n",
    "\n",
    "    yes_probs = token_probs[:, yes_token_ids].sum(dim=-1)#.tolist()\n",
    "    no_probs = token_probs[:, no_token_ids].sum(dim=-1)#.tolist()\n",
    "    yn_logits_reg = torch.cat([yes_probs.unsqueeze(1), no_probs.unsqueeze(1)], dim=1)\n",
    "    yn_probs_reg = [[(y/(y+n)).item(), (n/(y+n)).item()] for y, n in zip(yes_probs, no_probs)]\n",
    "    self_prompting_confidence_reg.append(yn_probs_reg[0][0])\n",
    "    logprobs_dict[\"self_prompting_conf_reg\"] = yn_probs_reg[0][0]\n",
    "    logprobs_dict[\"yn_logits_reg\"] = yn_logits_reg[0][0].cpu().numpy().tolist()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs_ = model.generate(inputs, \n",
    "        images=image_tensor_cd.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    logits = outputs_.scores[0]\n",
    "\n",
    "    token_probs = nn.Softmax(dim=-1)(logits)\n",
    "    yn_tokens = tokenizer.tokenize(\"yesYes Yesyesno noNo No\")\n",
    "    yes_probs = token_probs[:, yes_token_ids].sum(dim=-1)#.tolist()\n",
    "    no_probs = token_probs[:, no_token_ids].sum(dim=-1)#.tolist()\n",
    "    yn_logits_aug = torch.cat([yes_probs.unsqueeze(1), no_probs.unsqueeze(1)], dim=1)\n",
    "    yn_probs_aug = [[(y/(y+n)).item(), (n/(y+n)).item()] for y, n in zip(yes_probs, no_probs)]\n",
    "    logprobs_dict[\"self_prompting_diff\"] = yn_probs_aug[0][0]\n",
    "    logprobs_dict[\"yn_logits_diff\"] = yn_logits_aug[0][0].cpu().numpy().tolist()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs_ = model.generate(inputs, \n",
    "        images=image_tensor_cd2.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    logits = outputs_.scores[0]\n",
    "\n",
    "    token_probs = nn.Softmax(dim=-1)(logits)\n",
    "    yn_tokens = tokenizer.tokenize(\"yesYes Yesyesno noNo No\")\n",
    "    yes_probs = token_probs[:, yes_token_ids].sum(dim=-1)#.tolist()\n",
    "    no_probs = token_probs[:, no_token_ids].sum(dim=-1)#.tolist()\n",
    "    yn_logits_aug = torch.cat([yes_probs.unsqueeze(1), no_probs.unsqueeze(1)], dim=1)\n",
    "    yn_probs_aug = [[(y/(y+n)).item(), (n/(y+n)).item()] for y, n in zip(yes_probs, no_probs)]\n",
    "    logprobs_dict[\"self_prompting_black\"] = yn_probs_aug[0][0]\n",
    "    logprobs_dict[\"yn_logits_black\"] = yn_logits_aug[0][0].cpu().numpy().tolist()\n",
    "    # self_prompting_confidence_aug.append(yn_probs_aug[0][0])\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs_ = model.generate(inputs, \n",
    "        images=image_tensor_cd3.unsqueeze(0).to(dtype=torch.float16, device=device, non_blocking=True),\n",
    "        max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    logits = outputs_.scores[0]\n",
    "\n",
    "    token_probs = nn.Softmax(dim=-1)(logits)\n",
    "    yn_tokens = tokenizer.tokenize(\"yesYes Yesyesno noNo No\")\n",
    "    yes_probs = token_probs[:, yes_token_ids].sum(dim=-1)#.tolist()\n",
    "    no_probs = token_probs[:, no_token_ids].sum(dim=-1)#.tolist()\n",
    "    yn_logits_aug = torch.cat([yes_probs.unsqueeze(1), no_probs.unsqueeze(1)], dim=1)\n",
    "    yn_probs_aug = [[(y/(y+n)).item(), (n/(y+n)).item()] for y, n in zip(yes_probs, no_probs)]\n",
    "    logprobs_dict[\"self_prompting_attn\"] = yn_probs_aug[0][0]\n",
    "    logprobs_dict[\"yn_logits_attn\"] = yn_logits_aug[0][0].cpu().numpy().tolist()\n",
    "\n",
    "    directvqa_rollouts[i] = {\n",
    "        'qid': i,\n",
    "        'image_id': datum['image_id'],\n",
    "        'vqa_question': qs, \n",
    "        'annotated_answers': answers_dict,\n",
    "        'answer': answer, \n",
    "        'blackimage_answer': blackimage_answer,\n",
    "        'beam_answers': beam_answers,\n",
    "        'beam_logprobs': beam_logprobs,\n",
    "        'answer_logprobs_dict': logprobs_dict,\n",
    "        'blackimage_logprobs_dict': blackimage_logprobs_dict,\n",
    "        'attn_logprobs_dict': attn_logprobs_dict,\n",
    "        'diff_logprobs_dict': diff_logprobs_dict,\n",
    "        'distance_reg_black': distance_reg_black,\n",
    "        'distance_reg_attn': distance_reg_attn,\n",
    "        'distance_reg_diff': distance_reg_diff,\n",
    "        # 'score': float(score), \n",
    "        'lave_score': float(lave_score),\n",
    "        'lave_reasoning': lave_reasoning,\n",
    "    }\n",
    "    json.dump(directvqa_rollouts, open(answers_file, 'w'), indent=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
