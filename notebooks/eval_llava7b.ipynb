{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emushtaq/miniconda/envs/vcd/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../utils/reliability_utils\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../utils\")))\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "device = torch.device(\"cuda:\" + str(1) if torch.cuda.is_available() else \"cpu\")\n",
    "question_file = '/vault/erum/generations/llava7b_aokvqa_generation.json' #update the filename\n",
    "questions_file = open(os.path.expanduser(question_file), \"r\")\n",
    "data_dict = json.load(questions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUROC(confidence_scores, correctness):\n",
    "    auroc_score = metrics.roc_auc_score(1 - np.array(correctness),\n",
    "                                                            1-np.array(confidence_scores))\n",
    "    return auroc_score\n",
    "    # print(\"AUROC \"+label+\" \"+str(metrics.roc_auc_score(1 - np.array(correctness),\n",
    "    #                                                         1-np.array(confidence_scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/jlko/semantic_uncertainty\n",
    "class BaseEntailment:\n",
    "    def save_prediction_cache(self):\n",
    "        pass\n",
    "class EntailmentDeberta(BaseEntailment):\n",
    "    def __init__(self, DEVICE=\"cpu\"):\n",
    "        self.Device = DEVICE\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge-mnli\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"microsoft/deberta-v2-xlarge-mnli\").to(self.Device)\n",
    "\n",
    "    def check_implication(self, text1, text2, *args, **kwargs):\n",
    "        inputs = self.tokenizer(text1, text2, return_tensors=\"pt\").to(self.Device)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Deberta-mnli returns `neutral` and `entailment` classes at indices 1 and 2.\n",
    "        largest_index = torch.argmax(F.softmax(logits, dim=1))  # pylint: disable=no-member\n",
    "        prediction = largest_index.cpu().item()\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/jlko/semantic_uncertainty\n",
    "def get_semantic_ids(strings_list, model, strict_entailment=False, example=None):\n",
    "    \"\"\"Group list of predictions into semantic meaning.\"\"\"\n",
    "\n",
    "    def are_equivalent(text1, text2):\n",
    "\n",
    "        implication_1 = model.check_implication(text1, text2, example=example)\n",
    "        implication_2 = model.check_implication(text2, text1, example=example)  # pylint: disable=arguments-out-of-order\n",
    "        assert (implication_1 in [0, 1, 2]) and (implication_2 in [0, 1, 2])\n",
    "\n",
    "        if strict_entailment:\n",
    "            semantically_equivalent = (implication_1 == 2) and (implication_2 == 2)\n",
    "\n",
    "        else:\n",
    "            implications = [implication_1, implication_2]\n",
    "            # Check if none of the implications are 0 (contradiction) and not both of them are neutral.\n",
    "            semantically_equivalent = (0 not in implications) and ([1, 1] != implications)\n",
    "\n",
    "        return semantically_equivalent\n",
    "\n",
    "    # Initialise all ids with -1.\n",
    "    semantic_set_ids = [-1] * len(strings_list)\n",
    "    # Keep track of current id.\n",
    "    next_id = 0\n",
    "    for i, string1 in enumerate(strings_list):\n",
    "        # Check if string1 already has an id assigned.\n",
    "        if semantic_set_ids[i] == -1:\n",
    "            # If string1 has not been assigned an id, assign it next_id.\n",
    "            semantic_set_ids[i] = next_id\n",
    "            for j in range(i+1, len(strings_list)):\n",
    "                # Search through all remaining strings. If they are equivalent to string1, assign them the same id.\n",
    "                if are_equivalent(string1, strings_list[j]):\n",
    "                    semantic_set_ids[j] = next_id\n",
    "            next_id += 1\n",
    "\n",
    "    assert -1 not in semantic_set_ids\n",
    "\n",
    "    return semantic_set_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_assignment_entropy(semantic_ids):\n",
    "    \"\"\"Estimate semantic uncertainty from how often different clusters get assigned.\n",
    "\n",
    "    We estimate the categorical distribution over cluster assignments from the\n",
    "    semantic ids. The uncertainty is then given by the entropy of that\n",
    "    distribution. This estimate does not use token likelihoods, it relies soley\n",
    "    on the cluster assignments. If probability mass is spread of between many\n",
    "    clusters, entropy is larger. If probability mass is concentrated on a few\n",
    "    clusters, entropy is small.\n",
    "\n",
    "    Input:\n",
    "        semantic_ids: List of semantic ids, e.g. [0, 1, 2, 1].\n",
    "    Output:\n",
    "        cluster_entropy: Entropy, e.g. (-p log p).sum() for p = [1/4, 2/4, 1/4].\n",
    "    \"\"\"\n",
    "\n",
    "    n_generations = len(semantic_ids)\n",
    "    counts = np.bincount(semantic_ids)\n",
    "    probabilities = counts/n_generations\n",
    "    assert np.isclose(probabilities.sum(), 1)\n",
    "    entropy = - (probabilities * np.log(probabilities)).sum()\n",
    "    return entropy\n",
    "\n",
    "def predictive_entropy_rao(log_probs):\n",
    "    entropy = - np.sum(np.exp(log_probs) * log_probs)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized'):\n",
    "    \"\"\"Sum probabilities with the same semantic id.\n",
    "\n",
    "    Log-Sum-Exp because input and output probabilities in log space.\n",
    "    \"\"\"\n",
    "    unique_ids = sorted(list(set(semantic_ids)))\n",
    "    assert unique_ids == list(range(len(unique_ids)))\n",
    "    log_likelihood_per_semantic_id = []\n",
    "\n",
    "    for uid in unique_ids:\n",
    "        # Find positions in `semantic_ids` which belong to the active `uid`.\n",
    "        id_indices = [pos for pos, x in enumerate(semantic_ids) if x == uid]\n",
    "        # Gather log likelihoods at these indices.\n",
    "        id_log_likelihoods = [log_likelihoods[i] for i in id_indices]\n",
    "        if agg == 'sum_normalized':\n",
    "            # log_lik_norm = id_log_likelihoods - np.prod(log_likelihoods)\n",
    "            log_lik_norm = id_log_likelihoods - np.log(np.sum(np.exp(log_likelihoods)))\n",
    "            logsumexp_value = np.log(np.sum(np.exp(log_lik_norm)))\n",
    "        else:\n",
    "            raise ValueError\n",
    "        log_likelihood_per_semantic_id.append(logsumexp_value)\n",
    "\n",
    "    return log_likelihood_per_semantic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emushtaq/miniconda/envs/vcd/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = EntailmentDeberta(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import simps\n",
    "\n",
    "def compute_aurac(correctness, reliability):\n",
    "    # Ensure inputs are numpy arrays\n",
    "    correctness = np.array(correctness)\n",
    "    reliability = np.array(reliability)\n",
    "    \n",
    "    # Sort by reliability in descending order\n",
    "    sorted_indices = np.argsort(-reliability)\n",
    "    correctness = correctness[sorted_indices]\n",
    "    reliability = reliability[sorted_indices]\n",
    "    \n",
    "    # Cumulative sums for reliability and correctness\n",
    "    cumulative_reliability = np.cumsum(reliability) / np.sum(reliability)\n",
    "    cumulative_accuracy = np.cumsum(correctness) / np.arange(1, len(correctness) + 1)\n",
    "    \n",
    "    # Compute the area under the curve\n",
    "    aurac = simps(cumulative_accuracy, cumulative_reliability)\n",
    "    \n",
    "    return aurac\n",
    "\n",
    "def compute_aurac_with_penalty(correctness, reliability):\n",
    "    # Ensure inputs are numpy arrays\n",
    "    correctness = np.array(correctness)\n",
    "    reliability = np.array(reliability)\n",
    "    \n",
    "    # Sort by reliability in descending order\n",
    "    sorted_indices = np.argsort(-reliability)\n",
    "    correctness = correctness[sorted_indices]\n",
    "    reliability = reliability[sorted_indices]\n",
    "    \n",
    "    # Compute cumulative metrics with penalty\n",
    "    cumulative_correct = np.cumsum(correctness)  # Correct answers cumulative sum\n",
    "    cumulative_penalty = np.cumsum(1 - correctness)  # Incorrect answers cumulative sum\n",
    "    cumulative_accuracy = (cumulative_correct - cumulative_penalty) / np.arange(1, len(correctness) + 1)\n",
    "    \n",
    "    # Ensure cumulative accuracy remains within valid bounds\n",
    "    cumulative_accuracy = np.clip(cumulative_accuracy, 0, 1)\n",
    "    \n",
    "    # Cumulative reliability\n",
    "    cumulative_reliability = np.cumsum(reliability) / np.sum(reliability)\n",
    "    \n",
    "    # Compute the area under the curve\n",
    "    aurac = simps(cumulative_accuracy, cumulative_reliability)\n",
    "    \n",
    "    return aurac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/Ybakman/TruthTorchLM\n",
    "def prediction_rejection_curve(estimator, target):\n",
    "    \"\"\"\n",
    "    Calculates the prediction rejection curve score.\n",
    "    \n",
    "    The prediction rejection curve shows how model performance changes as we reject predictions\n",
    "    based on their uncertainty estimates.\n",
    "\n",
    "    Args:\n",
    "        estimator (array-like): Array of uncertainty estimates for each prediction\n",
    "        target (array-like): Array of true values/labels\n",
    "\n",
    "    Returns:\n",
    "        float: Prediction rejection curve score\n",
    "    \"\"\"\n",
    "    target = normalize(target) #higher is correct\n",
    "    # estimator: lower is more uncertain\n",
    "    ue = np.array(estimator)\n",
    "    num_obs = len(ue)\n",
    "    # Sort in descending order: the least uncertain come first\n",
    "    ue_argsort = np.argsort(ue)[::-1]\n",
    "    # want sorted_metrics to be increasing => smaller scores is better\n",
    "    sorted_metrics = np.array(target)[ue_argsort]\n",
    "    # Since we want all plots to coincide when all the data is discarded\n",
    "    cumsum = np.cumsum(sorted_metrics)[-num_obs:]\n",
    "    scores = (cumsum / np.arange(1, num_obs + 1))[::-1]\n",
    "    prr_score = np.sum(scores) / num_obs\n",
    "    return prr_score\n",
    "\n",
    "def normalize(target):\n",
    "    \"\"\"\n",
    "    Normalizes an array of values to the range [0,1].\n",
    "\n",
    "    Args:\n",
    "        target (array-like): Array of values to normalize\n",
    "\n",
    "    Returns:\n",
    "        array: Normalized values between 0 and 1\n",
    "    \"\"\"\n",
    "    min_t, max_t = np.min(target), np.max(target)\n",
    "    if np.isclose(min_t, max_t):\n",
    "        min_t -= 1\n",
    "        max_t += 1\n",
    "    target = (np.array(target) - min_t) / (max_t - min_t)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: confidence AUROC: 0.7491123572393192, PRC: 0.8759513950357724\n",
      "Metric: Entropy AUROC: 0.5960085142156994, PRC: 0.807482394072488\n",
      "Metric: SE AUROC: 0.7614119109584754, PRC: 0.8733835496477144\n",
      "Metric: Cluster Entropy AUROC: 0.6977788808099237, PRC: 0.8487278697389765\n",
      "Metric: Self eval AUROC: 0.7206787225099051, PRC: 0.8604434414039752\n",
      "Metric: First Token AUROC: 0.6939309382629928, PRC: 0.8122382986460082\n",
      "Metric: Visual UE AUROC: 0.7527434198170159, PRC: 0.8764201029903292\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import rel_entr\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(0) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "qids = list(data_dict.keys())\n",
    "abstained_qids = []\n",
    "confidence = []\n",
    "accQA = []\n",
    "N = 0\n",
    "accuracy = 0\n",
    "\n",
    "entropy_scores = []    \n",
    "semantic_entropy = []\n",
    "cluster_entropy_ = []   \n",
    "self_eval_ = []\n",
    "first_token_ = [] \n",
    "difference_ =[]\n",
    "exact_match = []\n",
    "for qid in qids:\n",
    "    N += 1\n",
    "    rollout = data_dict[qid]\n",
    "    question = rollout['vqa_question']\n",
    "    answer_tokens =  rollout['answer_logprobs_dict']['answer_tokens']\n",
    "    lave_score = rollout['lave_score']\n",
    "    if answer_tokens[0] == '':\n",
    "        reg_first_token = rollout['answer_logprobs_dict']['token_probs'][1]\n",
    "    else:\n",
    "        reg_first_token = rollout['answer_logprobs_dict'][\"first_token_prob\"]\n",
    "    most_likely_logprob = np.prod(rollout['answer_logprobs_dict']['token_probs'])**(1/len(rollout['answer_logprobs_dict']['token_probs']))\n",
    "    first_token_.append(reg_first_token)\n",
    "\n",
    "    aug_first_token = rollout['blackimage_logprobs_dict'][\"first_token_prob\"]\n",
    "    self_eval = rollout['answer_logprobs_dict'][\"yn_logits_reg\"]\n",
    "    self_eval_.append(self_eval)\n",
    "    difference_.append(abs(rollout['answer_logprobs_dict'][\"first_token_prob\"] - rollout['blackimage_logprobs_dict'][\"first_token_prob\"]))\n",
    "    beam_answers = rollout['beam_answers']\n",
    "    log_likelihoods = []\n",
    "    for beam in range(5):\n",
    "        log_likelihoods.append(np.sum(np.log(rollout['beam_logprobs'][beam]['token_probs'])))\n",
    "    beam_answers = rollout['beam_answers']\n",
    "\n",
    "    if lave_score > 0:\n",
    "        correctess_score = 1\n",
    "    else:\n",
    "        correctess_score = 0\n",
    "    accQA.append(correctess_score)\n",
    "\n",
    "\n",
    "    strings_list = [f'{question} {r}' for r in beam_answers[1:]]\n",
    "    semantic_ids = get_semantic_ids(strings_list, model, strict_entailment=False, example=None)\n",
    "\n",
    "    cluster_entropy = cluster_assignment_entropy(semantic_ids)\n",
    "    confidence.append(np.prod(most_likely_logprob))\n",
    "    entropy_scores.append(-predictive_entropy_rao(log_likelihoods))\n",
    "    log_probs = logsumexp_by_id(semantic_ids, log_likelihoods, agg='sum_normalized')\n",
    "    semantic_entropy.append(-predictive_entropy_rao(log_probs))\n",
    "    cluster_entropy_.append(-cluster_entropy)\n",
    "\n",
    "metrics_ = [\"confidence\", \"Entropy\", \"SE\", \"Cluster Entropy\", \"Self eval\", \"First Token\", \"Visual UE\"]\n",
    "scores = [confidence, entropy_scores, semantic_entropy, cluster_entropy_, self_eval_, first_token_, difference_]\n",
    "index = 0\n",
    "flattened_label = accQA\n",
    "RANDOM_SEED = 42\n",
    "for metric_name in metrics_:\n",
    "    flattened_scores = scores[index]\n",
    "    auroc = AUROC(flattened_scores, flattened_label)\n",
    "    prc = prediction_rejection_curve(flattened_scores, flattened_label)\n",
    "    print(f\"Metric: {metric_name} AUROC: {auroc}, PRC: {prc}\")\n",
    "    index += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
